{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO495 ASML Coursework 2 - Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coursework, you are asked to implement filtering, smoothing, and optionally Viterbi decoding for discrete and continuous valued HMMs. Input data and initialization is provided and should be used for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from scipy.stats import norm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are here to guide you in your implementation of the EM and Viterbi algorithms for Hidden Markov Models. We follow Section 17.4 of _Machine Learning: A Probabilistic Perspective_ by Kevin Murphy (2012).\n",
    "\n",
    "You should write vectorized modular code to promote re-usability, efficiency, and readability.\n",
    "\n",
    "Your task is to complete the implementation and to report the results obtained from the provided initialization. You are strongly encouraged to explore different initialization schemes for the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use this function in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def normalize(A, dim=None, precision=1e-9):\n",
    "#     \"\"\"This function is taken from Kevin Murphy's code for Machine Learning: a Probabilistic Perspective.\n",
    "\n",
    "#     Make the entries of a (multidimensional) array sum to 1\n",
    "#     A, z = normalize(A) normalize the whole array, where z is the normalizing constant\n",
    "#     A, z = normalize(A, dim)\n",
    "#     If dim is specified, we normalize the specified dimension only.\n",
    "#     dim=0 means each column sums to one\n",
    "#     dim=1 means each row sums to one\n",
    "\n",
    "\n",
    "#     Set any zeros to one before dividing.\n",
    "#     This is valid, since s=0 iff all A(i)=0, so\n",
    "#     we will get 0/1=0\n",
    "\n",
    "#     Adapted from https://github.com/probml/pmtk3\"\"\"\n",
    "#     z = A.sum(dim)\n",
    "#     # If z is a scalar, z.shape is an empty tuple and evaluated to False\n",
    "#     if z.shape:\n",
    "#         z[np.abs(z) < precision] = 1\n",
    "#     elif np.abs(z) < precision:\n",
    "#         return 0, 1\n",
    "    \n",
    "#     return A / z, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(A, dim=None, precision=1e-9):\n",
    "    \"\"\"This function is adapted from Kevin Murphy's code for Machine Learning: a Probabilistic Perspective.\n",
    "\n",
    "    Make the entries of a (multidimensional) array sum to 1\n",
    "    A, z = normalize(A) normalize the whole array, where z is the normalizing constant\n",
    "    A, z = normalize(A, dim)\n",
    "    If dim is specified, we normalize the specified dimension only.\n",
    "    dim=0 means each column sums to one\n",
    "    dim=1 means each row sums to one\n",
    "\n",
    "\n",
    "    Set any zeros to one before dividing.\n",
    "    This is valid, since s=0 iff all A(i)=0, so\n",
    "    we will get 0/1=0\n",
    "\n",
    "    Adapted from https://github.com/probml/pmtk3\"\"\"\n",
    "    \n",
    "    if dim is not None and dim > 1:\n",
    "        raise ValueError(\"Normalize doesn't support more than two dimensions.\")\n",
    "    \n",
    "    z = A.sum(dim)\n",
    "    # If z is a scalar, z.shape is an empty tuple and evaluates to False\n",
    "    if z.shape:\n",
    "        z[np.abs(z) < precision] = 1\n",
    "    elif np.abs(z) < precision:\n",
    "        return 0, 1\n",
    "    \n",
    "    if dim == 1:\n",
    "        return np.transpose(A.T / z), z\n",
    "    else:\n",
    "        return A / z, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values are provided as namedtuples (initialization.A is the initial value for A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "InitGaussian = namedtuple('InitGaussian', ['A', 'Means', 'Variances', 'pi'])\n",
    "InitMultinomial = namedtuple('InitMultinomial', ['A', 'B', 'pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break down your implementation according to the functions below. Feel free to create additional ones whenever you see fit, but the general flow of the algorithm should be made apparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of EM estimation on HMM operates on vectors of probabilities, so the main difference between EM for Gaussian HMM and multinomial HMM is the computation of the observation probabilities and which parameters to estimate.\n",
    "\n",
    "Complete the two functions below to compute the probabilities of the data for a given observation model and use them in the rest of the algorithm. Your filtering and smoothing steps should be model agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.stats import norm\n",
    "# f1=np.load('data_gaussian.npz')\n",
    "# f2=np.load('init_gaussian.npz')\n",
    "# #print f2['arr_0'],f2['arr_1'],f2['arr_2'],f2['arr_3']\n",
    "# Y_d=f1['arr_0']\n",
    "# #print Y_d\n",
    "def computeSmallB_Gaussian(Y, Means, Variances, Nhidden, T):\n",
    "    \"\"\"Compute the probabilities for the data points Y for a Gaussian observation model \n",
    "        with parameters Means and Variances.\n",
    "        \n",
    "        Input parameters:\n",
    "            - Y: the data\n",
    "            - Means: vector of the current estimates of the means\n",
    "            - Variances: vector of the current estimates of the variances\n",
    "            - Nhidden: number of hidden states\n",
    "            - T: length of the sequence\n",
    "        Output:\n",
    "            - b: vector of observation probabilities\n",
    "    \"\"\"\n",
    "    b=np.zeros((Nhidden,T))#here nh=2\n",
    "    for i in range(Nhidden):\n",
    "        for j in range(T):\n",
    "            b[i][j]=norm.pdf(Y[j],Means[i],Variances[i])\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# f=np.load('data_multinomial.npz')\n",
    "# Y_d= f['arr_0']\n",
    "# f= np.load('init_multinomial.npz')\n",
    "# B= f['arr_1']\n",
    "\n",
    "def computeSmallB_Discrete(Y, B):\n",
    "    \"\"\"Compute the probabilities for the data points Y for a multinomial observation model \n",
    "        with observation matrix B\n",
    "        \n",
    "        Input parameters:\n",
    "            - Y: the data\n",
    "            - B: matrix of observation probabilities\n",
    "        Output:\n",
    "            - b: vector of observation probabilities\n",
    "    \"\"\"\n",
    "    N = B.shape[0]\n",
    "    T = len(Y)\n",
    "    b = np.zeros((N,T))\n",
    "    for i in range(N):\n",
    "        for j in range(T):\n",
    "            b[i][j]=B[i][Y[j]-1]\n",
    "    return b \n",
    "# b= computeSmallB_Discrete(Y_d[1], B)[:,0]*np.array([1,1])\n",
    "# print  normalize(b, dim=None, precision=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing and filtering: Estimation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E step involves smoothing and filtering. Refer to the course notes and/or to the recommended readings to implement these steps in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def BackwardFiltering(A, b, N, T):\n",
    "    \"\"\"Perform backward filtering.\n",
    "        Input parameters:\n",
    "            - A: estimated transition matrix (between states)\n",
    "            - b: estimated observation probabilities (local evidence vector)\n",
    "            - N: number of hidden states\n",
    "            - T: length of the sequence\n",
    "        Output:\n",
    "            - beta: filtered probabilities\n",
    "    \"\"\"\n",
    "    beta=np.zeros((T,N))\n",
    "    beta[T-1]=np.ones(N)\n",
    "    t=T-2\n",
    "    while t>=0:\n",
    "        #beta[t]=np.dot(A,(beta[t+1]*b[:,t+1]))#N*N,N,N\n",
    "        beta[t]=normalize(np.dot(A,(beta[t+1]*b[:,t+1])),dim=None,precision=1e-9)[0]\n",
    "        t=t-1\n",
    "    return beta#beta t*n, save T vectors of z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ForwardFiltering(A, b, pi, N, T):\n",
    "    \"\"\"Filtering using the forward algorithm (Section 17.4.2 of K. Murphy's book)\n",
    "    Input:\n",
    "      - A: estimated transition matrix\n",
    "      - b: estimated observation probabilities (local evidence vector)\n",
    "      - pi: initial state distribution pi(j) = p(z_1 = j)\n",
    "    Output:\n",
    "      - Filtered belief state at time t: alpha = p(z_t|x_1:t)\n",
    "      - log p(x_1:T)\n",
    "      - Z: normalization constant\"\"\"\n",
    "    Z=np.zeros(T)\n",
    "    alpha=np.zeros((T,N))\n",
    "    alpha[0]=np.reshape(pi,2)*np.reshape(b[:,0],2)#shape(2,)\n",
    "    alpha[0],Z[0]=normalize(alpha[0], dim=None, precision=1e-9)\n",
    "    for t in range(T-1):\n",
    "        alpha[t+1],Z[t+1] =normalize(b[:,t]*np.dot(A.T,alpha[t]),dim=None,precision=1e-9)\n",
    "    logProb=math.log(normalize(alpha[T-1])[1])\n",
    "    for z in Z:\n",
    "        logProb=logProb+math.log(z)\n",
    "    return alpha, logProb, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ForwardBackwardSmoothing(A, b, pi, N, T):\n",
    "    \"\"\"Smoothing using the forward-backward algorithm.\n",
    "    Input:\n",
    "      - A: estimated transition matrix\n",
    "      - b: local evidence vector (observation probabilities)\n",
    "      - pi: initial distribution of states\n",
    "      - N: number of hidden states\n",
    "      - T: length of the sequence\n",
    "    Output:\n",
    "      - alpha: filtered belief state as defined in ForwardFiltering\n",
    "      - beta: conditional likelihood of future evidence as defined in BackwardFiltering\n",
    "      - gamma: gamma_t(j) proportional to alpha_t(j) * beta_t(j)\n",
    "      - lp: log probability defined in ForwardFiltering\n",
    "      - Z: constant defined in ForwardFiltering\"\"\"\n",
    "    alpha,logProb,Z=ForwardFiltering(A, b, pi, N, T)\n",
    "    beta=BackwardFiltering(A, b, N, T)\n",
    "    gamma=np.zeros((T,N))\n",
    "    for t in range(T):\n",
    "        gamma[t]=alpha[t]*beta[t]\n",
    "    return alpha, beta, gamma, logProb, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the output of SmoothedMarginals in the maximization step for A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def SmoothedMarginals(A, b, alpha, beta, T, Nhidden):\n",
    "    \"Two-sliced smoothed marginals p(z_t = i, z_t+1 = j | x_1:T)\"\n",
    "    \n",
    "    marginal = np.zeros((Nhidden, Nhidden, T-1));\n",
    "\n",
    "    for t in range(T-1):\n",
    "        marginal[:, :, t] = normalize(A * np.dot(np.reshape(alpha[t, :],(2,1)), np.reshape((b[:, t+1] * beta[t+1, :]),(1,2))))[0]\n",
    "    \n",
    "    return marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the main algorithm in the skeletons below.\n",
    "How can you measure the performance of your model and choose an appropriate convergence criterion?\n",
    "_Hint: the logProb returned by the ForwardBackwardSmoothing function can be used_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def EM_estimate_gaussian(Y, Nhidden, Niter, epsilon, init):\n",
    "    \n",
    "    # Dimensions of the data\n",
    "    N, T = Y.shape\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    # Initial transition matrix should be stochastic (rows sum to 1)\n",
    "    A = init.A\n",
    "    \n",
    "    # Initial means and variances of the emission probabilities\n",
    "    Means = init.Means\n",
    "    Variances = init.Variances;\n",
    "    \n",
    "    # Class prior\n",
    "    pi = init.pi\n",
    "    \n",
    "    ###############################################\n",
    "    # EM algorithm\n",
    "    \n",
    "    i = 0\n",
    "    # Initialize convergence criterion here\n",
    "    change = 10000#initialize a extremly large number\n",
    "    while i < Niter and change > epsilon: # and condition on criterion and precision epsilon\n",
    "        # Iterate here\n",
    "        print 'iterarion'\n",
    "        i=i+1\n",
    "        oldMeans = Means\n",
    "        oldVariances = Variances\n",
    "        means_num=np.zeros(Nhidden)\n",
    "        means_denum=np.zeros(Nhidden)\n",
    "        var_num=np.zeros(Nhidden)#here only one-dimension of data points is considered\n",
    "        var_denum=np.zeros(Nhidden)\n",
    "        pi_num=np.zeros(Nhidden)\n",
    "        pi_denum=np.zeros(Nhidden)\n",
    "        alpha = np.zeros((T,Nhidden))\n",
    "        beta = np.zeros((T,Nhidden))\n",
    "        #gamma = np.zeros((T,Nhidden))\n",
    "        b=np.zeros((Nhidden,T))\n",
    "        for k in range(Nhidden):\n",
    "            means_num = 0\n",
    "            means_denum = 0\n",
    "            for l in range(N):\n",
    "                b=computeSmallB_Gaussian(Y[l], oldMeans, oldVariances, Nhidden, T)\n",
    "                alpha, beta, gamma, logProb, Z=ForwardBackwardSmoothing(A, b, pi, Nhidden, T)\n",
    "                for t in range(T):\n",
    "                    means_num=means_num+gamma[t][k]*Y[l][t]\n",
    "                    means_denum=means_denum+gamma[t][k]\n",
    "            Means[k]=means_num / means_denum\n",
    "            \n",
    "#             for l in range(N):\n",
    "#                 b=computeSmallB_Gaussian(Y[l], oldMeans, oldVariances, Nhidden, T)\n",
    "#                 alpha, beta, gamma, logProb, Z=ForwardBackwardSmoothing(A, b, pi, Nhidden, T)\n",
    "#                 for t in range(T):\n",
    "#                     var_num[k]=var_num[k]+gamma[t][k]*(Y[l][t]-Means[k])**2\n",
    "#                     var_denum[k]=var_denum[k]+gamma[t][k]\n",
    "#             Variances[k]=var_num[k]/var_denum[k]\n",
    "            \n",
    "\n",
    "            \n",
    "#         change = np.dot((Means - oldMeans).T,(Means - oldMeans)) + np.dot((Variances-oldVariances).T,(Variances-oldVariances))\n",
    "        \n",
    "\n",
    "    return A, Means, Variances, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial observation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the maximization step for B you will have to compute a quantity involving indicators on the values of Y. One efficient way to do it is to pre-compute a representation of Y using _one-hot encoding_. In MATLAB:\n",
    "\n",
    "```% X sparse coding\n",
    "Nv = length(unique(Y));\n",
    "X = zeros(T, Nv);\n",
    "for i=1:T\n",
    "    X(i, Y(i)) = 1;\n",
    "end\n",
    "% Maximization: emission matrix\n",
    "B1 = B1 + gamma * X;```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def EM_estimate_multinomial(Y, Nhidden, Niter, epsilon, init):\n",
    "    \n",
    "    # Dimensions of the data\n",
    "    N, T = Y.shape\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    # Initial transition matrix should be stochastic (rows sum to 1)\n",
    "    A = init.A\n",
    "    \n",
    "    # Observation matrix B\n",
    "    B = init.B\n",
    "    \n",
    "    # Class prior\n",
    "    pi = init.pi\n",
    "    \n",
    "    ###############################################\n",
    "    # EM algorithm\n",
    "    \n",
    "    i = 0\n",
    "    # Initialize convergence criterion here\n",
    "    \n",
    "    while i<Niter: # and condition on criterion and precision epsilon\n",
    "        # Iterate here\n",
    "        break\n",
    "        \n",
    "    return A, B, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi decoding should be performed on the smoothed data and most of the algorithm doesn't depend on the output model. To help you, we identified the steps that are model specific. Implement Viterbi decoding by completing the skeleton below. 'smallB' is a function and should be used in the standard way: smallB(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ViterbiDecode(Y, Nhidden, outModel, init):\n",
    "    \n",
    "    if outModel == 'gauss':\n",
    "        A, Mu, Sigma, Pi = EM_estimate_gaussian(Y, Nhidden, 100, 1e-6, init)\n",
    "        smallB = lambda X : computeSmallB_Gaussian(X, Mu, Sigma, Nhidden, len(X))\n",
    "    elif outModel == 'multinomial':\n",
    "        A, B, Pi = EM_estimate_multinomial(Y, Nhidden, 100, 1e-6, init)\n",
    "        smallB = lambda X : computeSmallB_Discrete(X, B)\n",
    "    else:\n",
    "        raise ValueError('Invalid observation model: must be either \"gauss\" or \"multinomial\"')\n",
    "        \n",
    "    # Implement Viterbi decoding here.\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with np.load('init_gaussian.npz') as f:\n",
    "    init_g = InitGaussian(f['arr_0'], f['arr_1'], f['arr_2'], f['arr_3'])\n",
    "    \n",
    "with np.load('init_multinomial.npz') as f:\n",
    "    init_m = InitMultinomial(f['arr_0'], f['arr_1'], f['arr_2'])\n",
    "\n",
    "with np.load('data_gaussian.npz') as f:\n",
    "    Y_c, S_c = f['arr_0'], f['arr_1']\n",
    "\n",
    "with np.load('data_multinomial.npz') as f:\n",
    "    Y_d, S_d = f['arr_0'], f['arr_1']\n",
    "    \n",
    "\n",
    "\n",
    "A_g, Means_g, Variances_g, Pi_g = EM_estimate_gaussian(Y_c, 2, 10, 1e-6, init_g)\n",
    "A_m, B_m, Pi_m = EM_estimate_multinomial(Y_d, 2, 100, 1e-6, init_m)\n",
    "print A_g, Means_g, Variances_g, Pi_g\n",
    "\n",
    "# S_g = ViterbiDecode(Y_c, 2, 'gauss', init_g)\n",
    "# S_m = ViterbiDecode(Y_d, 2, 'multinomial', init_m)\n",
    "\n",
    "# print('*** Viterbi decoding accuracy (Gaussian): {}'.format( (S_c == S_g).sum() / S_c.size ))\n",
    "# print('*** Viterbi decoding accuracy (Multinomial): {}'.format( (S_d == S_m).sum() / S_d.size ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
